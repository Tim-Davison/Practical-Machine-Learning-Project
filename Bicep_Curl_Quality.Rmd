# On Recognizing the Quality of a Dumbell Bicep Curl  

By Tim Davison  
  
  July 9, 2017  
  

New technology such as Fitbit has enabled vast data collection of body movement. A study, "The Qualitative Activity Recognition of Weight Lifting" which was written by Wallace Ugulino, Eduardo Velloso, and Hugo Fuks examines how to recognize the quality of a specific exercise, the dumbbell bicep curl. The data used for this paper is available here: http://groupware.les.inf.puc-rio.br/har.   
 
Six men aged from 20 to 28 performed the bicep curl in 5 different manners. The first way was a correct curl, the next four were incorrect. The classe variable distinguishes the manner as such:  
 A. a correct dumbell bicep curl  
 B. throwing the elbow to the front  
 C. lifting only halfway  
 D. lowering only halfway  
 E. throwing the hips to the front.  
   
I first read in the files which were already separated into training and testing data sets. Those data files were found here:  
 training  
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csvhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
 
 testing  
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csvhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  
 
But, after looking at the variables names, I found that the variable classe (which is our outcome) is not included in the testing dataset. Also, many variables have a substantial amount of NA values. 
 
As the training dataset is very large with over 19,000 observations, I decided to split the training data into my training and testing data sets.   
This way I will be able to employ **cross validation** in my exploratory data analysis, and will develop regression models using the training set only. Once I have a final model I will test it on the testing data set only once.  
 
```{r, include=FALSE}
library(plyr); library(gbm); library(survival); library(splines);
library(dplyr); library(ggplot2); library(caret); library(randomForest); library(parallel); library(doMC); library(doParallel)
```  
But first I will minimize the large dataset by removing variables with many NA values, and those that are unnecessary.
```{r}  
setwd("~/DSC")  
training <- read.csv("pml-training.csv")
## to remove variables with large NA counts
na_count <-sapply(training, function(y) sum(length(which(is.na(y)))))
na_count <- as.data.frame(na_count)
na_count2 <- cbind(c(1:160), na_count)
na_count2 <- arrange(na_count2, desc(na_count))
naVarRemove <- na_count2[1:67,1]
training2 <- training[,-c(naVarRemove)]
## to remove "kurtosis", "yaw", and other "timestamp" variables
training3 <- training2[,-c(1:7,12:17,43:48,52:57,74:79)]
head(names(training3), 8)
```
Now I have reduced the 160 variable set down to just 62. After creating my data partion, I will explore various variables to see if they might be good predictors.  
```{r}
inTrain <- createDataPartition(y=training3$classe, p=.7, list=FALSE)
train <- training3[inTrain, ]
test <- training3[-inTrain, ]
dim(train); dim(test)
```  
We still have a large number of variables. I considered the movement/action of the error, and tried to find logical variables to distinguish them from each other and the correct curl. For the B error, throwing the elbow to the front, I looked for variables that would show movement of the arm. The C and D errors are similar (lifting halfway and lowering halfway). I looked for forearm movement for C and D. Since the E error is throwing hips to the front, I looked at variables associated with the belt.  
 
For each error, I gathered 5 to 10 potential variables and plotted them to see if they showed differentiation from A or all other errors.  
 
Here is a box plot of the variable "roll_arm":
 
```{r, fig.height=3.5, fig.width=3.5}
qplot(classe, roll_arm, data=training, geom="boxplot", color=I("steelblue"))
```

In this plot we see that the B error stands out from the rest of the pack with a considerably higher median value. Another variable I used is "total_accel_belt". In this density curve we see clear differentiation of the E error.

```{r, fig.height=3.5, fig.width=4.5}
qplot(total_accel_belt, color=classe, data=training, geom="density")
```  
  
This is one of the "belt" variables I used to distinquish the E error, throwing the hips. After plotting dozens of variables, I narrowed down my variables to just five.  
```{r}
print(names(train[,c(4,17,47,59,27)]))  

```  

Next I ran these 5 variables through several regression models to see which would be the most accurate. At first my old MacBook Pro was having difficulty, and sometimes completely shutting down. My results were not great either. I experimented with some trainControl settings, and brought down my computation time. I was then able to add three more variables, which brought up my results considerably.  

```{r}
print(names(training3[,c(1,15,40)]))  
```  
After adding these three new variables the random forest model was fairly accurate:   
```{r, results="hide", cache=TRUE }
set.seed(55501)
fitControl <- trainControl(method = "cv", number = 7, trim=TRUE)

mod1 <- train(classe~total_accel_belt + roll_arm + pitch_forearm +
            magnet_arm_x + magnet_forearm_x +
            roll_belt + magnet_belt_y + accel_dumbbell_x,
            data=train, method="rf", trControl = fitControl)  
```  
  
```{r}
confusionMatrix(mod1)  
```  

In the confusion matrix we see 94.85% accuracy. Let's now run the same set of variables through knn and gbm models.  

```{r, results="hide", cache=TRUE}
set.seed(55502)
fitControl2 <- trainControl(method= "repeatedcv",number=9, 
                    repeats=3, trim=TRUE)
mod2 <- train(classe~total_accel_belt + roll_arm + pitch_forearm +
            magnet_arm_x + magnet_forearm_x + roll_belt + 
            magnet_belt_y + accel_dumbbell_x, data=train, method="knn",
            tuneLength=12, trControl = fitControl2)  
```  
  
```{r}
confusionMatrix(mod2)  
```  
The knn model has around 69.8% accuracy, and the rf model has 94.9% accuracy. Let's look at one more model, gbm.  
```{r, results="hide", cache=TRUE}
set.seed(55503)
fitControl3 <- trainControl(method = "cv", number = 7, 
                    verboseIter = FALSE, trim=TRUE)

mod3 <- train(classe~total_accel_belt + roll_arm + pitch_forearm +
            magnet_arm_x + magnet_forearm_x + roll_belt + 
            magnet_belt_y + accel_dumbbell_x, data=train, 
            method="gbm", trControl = fitControl3)  
```

```{r}
confusionMatrix(mod3)  
```  
Gbm is better than knn, but not as good as rf. Now let's combine all three models to see if the results improve. Here are the first 5 rows of my combined dataframe:  
```{r}
pred1 <- predict(mod1, test)
pred2 <- predict(mod2, test)
pred3 <- predict(mod3, test)
predDF <- data.frame(pred1, pred2, pred3, classe=test$classe)
head(predDF)  
```  
Now I will train the combined data and show the results.  
```{r, include=FALSE, results="hide", cache=TRUE}
set.seed(55504)
fitControl4 <- trainControl(method = "cv", number = 7, 
                    verboseIter = FALSE, trim=TRUE)
combModFit <- train(classe~., method="gbm", data=predDF, 
                    trControl = fitControl4)
```  
```{r}
confusionMatrix(combModFit)
```  
Our final model is at 95% accuracy. I am happy with this result. The **expected out of sample error** is 1 - accuracy in the cross validation test set. Since my accuracy is 95%, I expect my error rate to be slightly higher than 5%. I think it will be around 6-7%.

End







 
 